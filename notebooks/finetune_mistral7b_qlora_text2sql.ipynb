{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Mistral-7B-Instruct with QLoRA for Text-to-SQL\n",
    "\n",
    "This notebook walks through **end-to-end fine-tuning** of\n",
    "`mistralai/Mistral-7B-Instruct-v0.1` for **Text-to-SQL** using:\n",
    "\n",
    "- The preprocessed instruction-tuning dataset in `data/processed/*.jsonl`.\n",
    "- **QLoRA** (4-bit quantization + LoRA adapters).\n",
    "- **Unsloth** + **bitsandbytes** for efficient training.\n",
    "- **TRL**'s `SFTTrainer` for supervised fine-tuning.\n",
    "\n",
    "We assume you are running this in a GPU environment (e.g., Google Colab).\n",
    "\n",
    "## What you will learn\n",
    "\n",
    "1. How QLoRA works conceptually and why it is well-suited for 7B models.\n",
    "2. How schema-grounded prompts (including `CREATE TABLE` context) help\n",
    "   reduce schema hallucinations in Text-to-SQL.\n",
    "3. How to load and inspect the preprocessed dataset.\n",
    "4. How to format prompts for supervised fine-tuning.\n",
    "5. How to configure and run a QLoRA training loop using Unsloth + TRL.\n",
    "6. How to save adapters and run a quick sanity check on generated SQL.\n",
    "7. What the next steps are for external validation and deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Overview: QLoRA + Schema-Grounded Prompts\n",
    "\n",
    "**QLoRA** (Quantized LoRA) combines:\n",
    "\n",
    "- **4-bit quantization** of the base model weights (via bitsandbytes).\n",
    "- **Low-Rank Adapters (LoRA)** that are trained while keeping the\n",
    "  quantized base model frozen.\n",
    "\n",
    "This dramatically reduces memory usage, allowing us to fine-tune 7B models\n",
    "on a single GPU while still achieving strong performance.\n",
    "\n",
    "For **Text-to-SQL**, we want the model to be:\n",
    "\n",
    "- **Schema-aware**: It should use only the tables and columns that actually\n",
    "  exist.\n",
    "- **Grounded**: The model should see the schema (e.g., `CREATE TABLE`)\n",
    "  alongside the question.\n",
    "- **Cautious**: Avoid hallucinating non-existent tables/columns.\n",
    "\n",
    "To support this, our preprocessing pipeline constructs prompts like:\n",
    "\n",
    "```text\n",
    "### Instruction:\n",
    "Write a SQL query that answers the user's question using ONLY the tables and columns provided in the schema.\n",
    "\n",
    "### Input:\n",
    "### Schema:\n",
    "CREATE TABLE head (age INTEGER)\n",
    "\n",
    "### Question:\n",
    "How many heads of the departments are older than 56 ?\n",
    "\n",
    "### Response:\n",
    "SELECT COUNT(*) FROM head WHERE age > 56\n",
    "```\n",
    "\n",
    "The **instruction** is fixed, the **input** combines schema + question, and\n",
    "the **response** is the target SQL. We will train the model to map\n",
    "instruction+input to response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Environment Setup\n",
    "\n",
    "This section installs dependencies (for Colab), checks GPU availability, and\n",
    "sets random seeds for reproducibility.\n",
    "\n",
    "> If you are running locally with the repository already installed, you may\n",
    "> skip the `pip install` step and simply import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "env-setup"
   },
   "outputs": [],
   "source": [
    "# If running in Colab, uncomment the following lines to install dependencies.\n",
    "# !pip install -q unsloth bitsandbytes accelerate transformers datasets trl peft\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / \"data\" / \"processed\"\n",
    "OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "print(f\"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"WARNING: CUDA not available. Training a 7B model will not be feasible on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Load Processed Dataset (train/val JSONL)\n",
    "\n",
    "The preprocessing step (`scripts/build_dataset.py`) produces two files:\n",
    "\n",
    "- `data/processed/train.jsonl`\n",
    "- `data/processed/val.jsonl`\n",
    "\n",
    "Each line is an Alpaca-style record with keys: `id`, `instruction`, `input`,\n",
    "`output`, `source`, and `meta`.\n",
    "\n",
    "We will load them into `datasets.Dataset` objects for use with TRL's\n",
    "`SFTTrainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-dataset"
   },
   "outputs": [],
   "source": [
    "def load_alpaca_jsonl(path: Path) -> Dataset:\n",
    "    if not path.is_file():\n",
    "        raise FileNotFoundError(f\"JSONL file not found: {path}\")\n",
    "    records = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            records.append(json.loads(line))\n",
    "    return Dataset.from_list(records)\n",
    "\n",
    "train_path = DATA_DIR / \"train.jsonl\"\n",
    "val_path = DATA_DIR / \"val.jsonl\"\n",
    "\n",
    "train_raw = load_alpaca_jsonl(train_path)\n",
    "val_raw = load_alpaca_jsonl(val_path)\n",
    "\n",
    "print(train_raw)\n",
    "print(val_raw)\n",
    "\n",
    "train_raw[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Prompt Formatting\n",
    "\n",
    "Our training format combines `instruction`, `input`, and `output` into a\n",
    "single `text` field. The prompt structure is:\n",
    "\n",
    "```text\n",
    "### Instruction:\n",
    "<instruction>\n",
    "\n",
    "### Input:\n",
    "<schema + question>\n",
    "\n",
    "### Response:\n",
    "<SQL output>\n",
    "```\n",
    "\n",
    "We will implement two helpers mirroring the library code in\n",
    "`src/text2sql/training/formatting.py`:\n",
    "\n",
    "- `build_prompt(instruction, input)` – builds the instruction + input + response header.\n",
    "- `ensure_sql_only(output)` – normalizes whitespace and strips code fences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prompt-formatting"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\"\"\n",
    "\n",
    "def build_prompt(instruction: str, input_text: str) -> str:\n",
    "    instruction = (instruction or \"\").strip()\n",
    "    input_text = (input_text or \"\").strip()\n",
    "    return PROMPT_TEMPLATE.format(instruction=instruction, input=input_text)\n",
    "\n",
    "def ensure_sql_only(output: str) -> str:\n",
    "    if output is None:\n",
    "        return \"\"\n",
    "    text = output.strip()\n",
    "    if text.startswith(\"```\"):\n",
    "        text = re.sub(r\"^```(?:sql)?\\s*\", \"\", text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r\"\\s*```$\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def format_example(example):\n",
    "    prompt = build_prompt(example[\"instruction\"], example[\"input\"])\n",
    "    sql = ensure_sql_only(example[\"output\"])\n",
    "    return {\"text\": prompt + sql}\n",
    "\n",
    "formatted_sample = format_example(train_raw[0])\n",
    "print(formatted_sample[\"text\"][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply this formatting to the entire dataset. During development,\n",
    "it is often useful to use a **fast dev run** on a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "format-dataset"
   },
   "outputs": [],
   "source": [
    "FAST_DEV_RUN = True  # Set to False for full training\n",
    "MAX_DEV_SAMPLES = 512\n",
    "\n",
    "if FAST_DEV_RUN:\n",
    "    train_raw_sliced = train_raw.select(range(min(MAX_DEV_SAMPLES, train_raw.num_rows)))\n",
    "    val_raw_sliced = val_raw.select(range(min(MAX_DEV_SAMPLES, val_raw.num_rows)))\n",
    "else:\n",
    "    train_raw_sliced = train_raw\n",
    "    val_raw_sliced = val_raw\n",
    "\n",
    "train_ds = train_raw_sliced.map(format_example, remove_columns=train_raw_sliced.column_names)\n",
    "val_ds = val_raw_sliced.map(format_example, remove_columns=val_raw_sliced.column_names)\n",
    "\n",
    "print(train_ds)\n",
    "print(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Load Base Model in 4-Bit & Apply LoRA Adapters\n",
    "\n",
    "We now load `mistralai/Mistral-7B-Instruct-v0.1` using Unsloth's\n",
    "`FastLanguageModel` in 4-bit mode and attach LoRA adapters.\n",
    "\n",
    "Key hyperparameters:\n",
    "\n",
    "- `r` (LoRA rank): capacity of the adapter (typical range: 8–64).\n",
    "- `alpha`: scale factor for the adapter.\n",
    "- `dropout`: dropout applied to the adapter; often 0.0 or small.\n",
    "- `target_modules`: which projection layers receive LoRA (q_proj, k_proj, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-model"
   },
   "outputs": [],
   "source": [
    "BASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.0\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"CUDA is not available. The following cell will likely OOM on CPU.\")\n",
    "\n",
    "dtype = None  # Let Unsloth choose bf16/fp16\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_R,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Training Loop with TRL's SFTTrainer\n",
    "\n",
    "We will now configure and run supervised fine-tuning with TRL's `SFTTrainer`.\n",
    "\n",
    "For **fast dev runs**, we will:\n",
    "\n",
    "- Limit the number of steps (e.g., `max_steps=20`).\n",
    "- Use a small subset of the data.\n",
    "\n",
    "Once everything works, you can disable `FAST_DEV_RUN` and increase\n",
    "`max_steps` for a proper training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-config"
   },
   "outputs": [],
   "source": [
    "MAX_STEPS = 20 if FAST_DEV_RUN else 500\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "LEARNING_RATE = 2e-4\n",
    "WARMUP_STEPS = 50\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    max_steps=MAX_STEPS,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=max(MAX_STEPS // 10, 1),\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=max(MAX_STEPS // 10, 1),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    dataset_text_field=\"text\",\n",
    "    args=sft_config,\n",
    ")\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-training"
   },
   "outputs": [],
   "source": [
    "train_output = trainer.train()\n",
    "train_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G. Save Artifacts (Adapters + Metrics)\n",
    "\n",
    "After training, we save the LoRA adapters and simple metrics:\n",
    "\n",
    "- `outputs/adapters/` – adapter weights and tokenizer config.\n",
    "- `outputs/run_meta.json` – run configuration and dataset sizes.\n",
    "- `outputs/metrics.json` – train/eval losses and related metrics (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-artifacts"
   },
   "outputs": [],
   "source": [
    "adapters_dir = OUTPUT_DIR / \"adapters\"\n",
    "adapters_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "trainer.model.save_pretrained(str(adapters_dir))\n",
    "tokenizer.save_pretrained(str(adapters_dir))\n",
    "\n",
    "# Save run metadata\n",
    "run_meta = {\n",
    "    \"base_model\": BASE_MODEL,\n",
    "    \"max_steps\": MAX_STEPS,\n",
    "    \"per_device_train_batch_size\": PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"max_seq_length\": MAX_SEQ_LENGTH,\n",
    "    \"lora_r\": LORA_R,\n",
    "    \"lora_alpha\": LORA_ALPHA,\n",
    "    \"lora_dropout\": LORA_DROPOUT,\n",
    "    \"fast_dev_run\": FAST_DEV_RUN,\n",
    "    \"num_train_examples\": len(train_ds),\n",
    "    \"num_val_examples\": len(val_ds),\n",
    "}\n",
    "\n",
    "with (OUTPUT_DIR / \"run_meta.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(run_meta, f, indent=2)\n",
    "\n",
    "# Save metrics if available\n",
    "metrics = {}\n",
    "if hasattr(train_output, \"metrics\") and train_output.metrics is not None:\n",
    "    metrics.update(train_output.metrics)\n",
    "\n",
    "try:\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    metrics.update({f\"eval_{k}\": v for k, v in eval_metrics.items()})\n",
    "except Exception as e:  # noqa: BLE001\n",
    "    print(\"Evaluation failed or was skipped:\", e)\n",
    "\n",
    "with (OUTPUT_DIR / \"metrics.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H. Quick Sanity Inference\n",
    "\n",
    "Finally, we perform a quick sanity check by generating SQL for a few\n",
    "examples. This is a qualitative check to see if the model learned to\n",
    "produce reasonable SQL given the schema and question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sanity-inference"
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def generate_sql(example, max_new_tokens: int = 128):\n",
    "    prompt = build_prompt(example[\"instruction\"], example[\"input\"])\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "\n",
    "    print(\"=== Prompt ===\")\n",
    "    print(prompt)\n",
    "    print(\"=== Generated SQL ===\")\n",
    "    _ = model.generate(\n",
    "        **inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    print(\"\\n=================\\n\")\n",
    "\n",
    "for i in range(3):\n",
    "    generate_sql(train_raw[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Next Steps\n",
    "\n",
    "You now have a QLoRA-finetuned Mistral-7B-Instruct model for Text-to-SQL.\n",
    "The next steps for this project are:\n",
    "\n",
    "1. **External Validation on Spider dev (planned Task 4)**\n",
    "   - Evaluate the model on a harder, multi-table, cross-domain benchmark\n",
    "     such as `xlangai/spider`.\n",
    "   - Measure logical form accuracy and execution accuracy.\n",
    "   - See `docs/external_validation.md` for the high-level plan.\n",
    "\n",
    "2. **Push to Hugging Face Hub (planned Task 5)**\n",
    "   - Package the LoRA adapters and associated config.\n",
    "   - Publish a model card documenting training data, metrics, and usage.\n",
    "\n",
    "3. **Streamlit Remote Inference UI (planned Task 6)**\n",
    "   - Integrate the fine-tuned model into a Streamlit app.\n",
    "   - Allow users to connect to a database, ask natural-language questions,\n",
    "     and inspect / edit the generated SQL.\n",
    "\n",
    "These steps will complete the full loop from data → training → evaluation →\n",
    "interactive Analytics Copilot experience."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}